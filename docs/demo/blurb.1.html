<details>
  <summary><strong>TL;DR: What's this?</strong></summary>
  <p>
    Construct a <em>lemma graph</em>, then perform <em>entity linking</em> based on:
    <code>spaCy</code>, <code>transformers</code>, <code>SpanMarkerNER</code>,
    <code>spaCy-DBpedia-Spotlight</code>, <code>REBEL</code>, <code>OpenNRE</code>,
    <code>spaCy fishing</code>, <code>qwikidata</code>
  </p>
  <p>
    This is a prelude step before leveraging
    <em>topological transforms</em>, <em>large language models</em>, <em>graph representation learning</em>,
    plus <em>human-in-the-loop</em> domain expertise to infer
    the nodes, edges, properties, and probabilities needed for the
    semi-automated construction of a <em>knowledge graph</em> from
    raw unstructured text sources.
  </p>
  <p>
    In addition to providing a library for production use cases,
    the <code>TextGraph</code> project creates a "playground" or "gym"
    in which to prototype and evaluate abstractions based on
    <a href="https://blog.derwen.ai/graph-levels-of-detail-ea4226abba55" target="_blank">"Graph Levels Of Detail"</a>.
  </p>
  <ol>
    <li>use <code>spaCy</code> to parse a document, with <code>SpanMarkerNER</code> LLM assist</li>
    <li>add noun chunks in parallel to entities, as "candidate" phrases for subsequent HITL</li>
    <li>perform <em>entity linking</em>: <code>spaCy-DBpedia-Spotlight</code>, <code>spaCy fishing</code></li>
    <li>infer relations, plus graph inference: <code>REBEL</code>, <code>OpenNRE</code>, <code>qwikidata</code></li>
    <li>build a <em>lemma graph</em> in <code>NetworkX</code> from the parse results</li>
    <li>run a modified <code>textrank</code> algorithm plus graph analytics</li>
    <li>approximate a <em>pareto archive</em> (hypervolume) to re-rank extracted entities</li>
    <li>visualize the <em>lemma graph</em> interactively in <code>PyVis</code></li>
    <li>cluster communities within the <em>lemma graph</em>
    <li>apply topological transforms to enhance embeddings (in progress)</li>
    <li>run graph representation learning on the <em>graph of relations</em> (in progress)</li>
  </ol>
  <p>
    ...
  </p>
  <ol start="13">
    <li>PROFIT!</li>
  </ol>
</details>


<details>
  <summary><strong>More details...</strong></summary>
  <p>
    Implementation of an LLM-augmented <code>textgraph</code> algorithm for
    constructing a <em>lemma graph</em> from raw, unstructured text source.
  </p>
  <p>
    The <code>textgraph</code> library is based on work developed by
    <a href="https://derwen.ai/graph" target="_blank">Derwen</a>
    in 2023 Q2 for customer apps and used in our <code>Cysoni</code>
    product.
    This demo integrates code from:
  </p>
  <ul>
    <li>
      <a href="https://github.com/tomaarsen/SpanMarkerNER/" target="_blank"><code>SpanMarkerNER</code></a>
    </li>
    <li>
      <a href="https://github.com/MartinoMensio/spacy-dbpedia-spotlight" target="_blank"><code>spaCy-DBpedia-Spotlight</code></a>
    </li>
    <li>
      <a href="https://github.com/Babelscape/rebel" target="_blank"><code>REBEL</code></a>
    </li>
    <li>
      <a href="https://github.com/thunlp/OpenNRE/" target="_blank"><code>OpenNRE</code></a>
    </li>
    <li>
      <a href="https://github.com/Lucaterre/spacyfishing" target="_blank"><code>spaCy fishing</code></a>
    </li>
    <li>
      <a href="https://github.com/kensho-technologies/qwikidata" target="_blank"><code>qwikidata</code></a>
    </li>
    <li>
      <a href="https://spacy.io/" target="_blank"><code>spaCy</code></a>
    </li>
    <li>
      <a href="https://huggingface.co/docs/transformers/index" target="_blank"><code>HF transformers</code></a>
    </li>
    <li>
      <a href="https://github.com/DerwenAI/pytextrank/" target="_blank"><code>PyTextRank</code></a>
    </li>
  </ul>

  <p>
    For more details about this approach, see these talks:
  </p>
  <ul>
    <li>
      <a href="https://derwen.ai/s/mqqm" target="_blank">"Language, Graphs, and AI in Industry"</a>
      <br/>
      <strong>Paco Nathan</strong>, K1st World (2023-10-11)
    </li>
    <li>
      <a href="https://derwen.ai/s/rhvg" target="_blank">"Language Tools for Creators"</a>
      <br/>
      <strong>Paco Nathan</strong>, FOSSY (2023-07-13)
    </li>
  </ul>

  <p>
    Other good tutorials (during 2023) which include related material:
  </p>
  <ul>
    <li>
      <a href="https://youtu.be/C9p7suS-NGk?si=7Ohq3BV654ia2Im4" target="_blank">"Natural Intelligence is All You Need™"</a>
      <br/>
      <strong>Vincent Warmerdam</strong>, PyData Amsterdam (2023-09-15)
    </li>
    <li>
      <a href="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a" target="_blank">"How to Convert Any Text Into a Graph of Concepts"</a>
      <br/>
      <strong>Rahul Nayak</strong>, <em>Towards Data Science</em> (2023-11-09)
    </li>
    <li>
      <a href="https://medium.com/@nizami_muhammad/extracting-relation-from-sentence-using-llm-597d0c0310a8" target="_blank">"Extracting Relation from Sentence using LLM"</a>
      <br/>
      <strong>Muhammad Nizami</strong> <em>Medium</em> (2023-11-15)
    <li>
  </ul>
</details>


<details>
  <summary><strong>Bibliography...</strong></summary>

  <p>
    "Automatic generation of hypertext knowledge bases"
    <br/>
    <strong>Udo Hahn</strong>, <strong>Ulrich Reimer</strong>
    <br/>
    <em>ACM SIGOIS</em> 9:2 (1988-04-01)
    <br/>
    <a href="https://doi.org/10.1145/966861.45429" target="_blank">https://doi.org/10.1145/966861.45429</a>
  </p>
  <blockquote>
    The condensation process transforms the text representation structures resulting from the text parse into a more abstract thematic description of what the text is about, filtering out irrelevant knowledge structures and preserving only the most salient concepts.
  </blockquote>

  <p>
    <em>Graph Representation Learning</em>
    <br/>
    <strong>William Hamilton</strong>
    <br/>
    Morgan and Claypool (pre-print 2020)
    <br/>
    <a href="https://www.cs.mcgill.ca/~wlh/grl_book/" target="_blank">https://www.cs.mcgill.ca/~wlh/grl_book/</a>
  </p>
  <blockquote>
    A brief but comprehensive introduction to graph representation learning, including methods for embedding graph data, graph neural networks, and deep generative models of graphs.
  </blockquote>

  <p>
    "RED<sup>FM</sup>: a Filtered and Multilingual Relation Extraction Dataset"
    <br/>
    <strong>Pere-Lluís Huguet Cabot</strong>, <strong>Simone Tedeschi</strong>, <strong>Axel-Cyrille Ngonga Ngomo</strong>, <strong>Roberto Navigli</strong>
    <br/>
    <em>ACL</em> (2023-06-19)
    <br/>
    <a href="https://arxiv.org/abs/2306.09802" target="_blank">https://arxiv.org/abs/2306.09802</a>
  </p>
  <blockquote>
    Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems.
  </blockquote>

  <p>
    "InGram: Inductive Knowledge Graph Embedding via Relation Graphs"
    <br/>
    <strong>Jaejun Lee</strong>, <strong>Chanyoung Chung</strong>, <strong>Joyce Jiyoung Whang</strong>
    <br/>
    <em>ICML</em> (2023–08–17)
    <br/>
    <a href="https://arxiv.org/abs/2305.19987" target="_blank">https://arxiv.org/abs/2305.19987</a>
  </p>
  <blockquote>
    In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time.
  </blockquote>

  <p>
    "TextRank: Bringing Order into Text"
    <br/>
    <strong>Rada Mihalcea</strong>, <strong>Paul Tarau</strong>
    <br/>
    <em>EMNLP</em> (2004-07-25)
    <br/>
    <a href="https://aclanthology.org/W04-3252" target="_blank">https://aclanthology.org/W04-3252</a>
  </p>
  <blockquote>
    In this paper, the authors introduce TextRank, a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.
  </blockquote>

</details>

<hr/>
